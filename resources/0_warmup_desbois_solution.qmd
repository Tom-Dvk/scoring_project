---
title: "Warm up - The Desbois Case study"
author: "Your NAME"
bibliography: references_intro.bib
csl: apa.csl
link-citations: true
format:
  html:
    theme: 
       light: cerulean
    # theme: darkly
    # highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    include-in-header: mathjax.html
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
    toc-location: left
    fontsize: 10pt
    mainfont: "Helvetica Neue"
execute: 
  #cache: true
  warning: false
editor: visual
fontsize: 11pt
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

## Desbois case study

Loading Desbois data and first glimpse at it:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

library(tidyverse)
# package used to import spss file
library(foreign)

don_desbois <- read.spss("presentation_data/Agriculture Farm Lending/desbois.sav", to.data.frame = TRUE) %>% as_tibble()
glimpse(don_desbois)
```

Replacing for convenience `DIFF` target by factor `Y` with `O` (healthy) or `1` (failing):

```{r}
#| echo: true
#| code-fold: show
#| warning: false

don_desbois <- don_desbois %>%
    mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1)), DIFF = NULL,
           .before = everything())

```

Your turn to play

Using the data set at hand, try to retrieve some findings of the Desbois case study.

You might need `R` packages `FactoMineR`, `ROCR` and `MASS::lda` function to perform the tasks ("YOUR CODE HERE") described in the following slides.

Each time I show an example of what is expected.

I suggest that you start using `Quarto` (setup [here](https://quarto.org/docs/get-started/)) to code and track/publish your results. It will be requested for your projects (it is very similar to `RMarkdown`). It integrates perfectly with `RStudio`.

Principal Component Analysis (PCA) of financial ratios

First perform PCA on financial ratios (use for example package [FactoMineR](http://factominer.free.fr/factomethods/principal-components-analysis.html)), then display correlations between ratios and the two first principal components :

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
#| code-fold: true
# More details here http://factominer.free.fr/factomethods/principal-components-analysis.html
res.pca = FactoMineR::PCA(don_desbois,
                          scale.unit = TRUE,
                          quanti.sup = c(4, 7), # HECTARE / AGE excluded from Desbois analysis
                          quali.sup = c(1, 2, 3, 5, 6, 8), 
                          ncp = 5,
                          graph=FALSE)
plot(res.pca, choix = "var")
# FactoMineR::dimdesc(res.pca, axes=c(1,2))
```

To be compared with article Figure 1

![](images/desbois_pca_projection_pc1_pc2.png){.r-stretch}

Following Desbois path, visualize the farm holdings in data set as a bivariate plot on the first two components of PCA (based on financial ratios), use variable Y (0=”healthy”; 1=”failing”) to colour your observations:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
#| code-fold: true
# Similar to Desbois Fig 2. 
# Plot of the farm holdings in the first factorial plane of the normalized PCA based on financial ratios
# with illustrative variable Y (0=”healthy”; 1=”failing”)
FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=1, invisible = c("quali"))

# FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=1, invisible = c("ind"))
```

To be compared with article Figure 2

It strikes Desbois that the PCA (even if not a discriminative/scoring procedure), seems to do a rather good job identifying defaulting farms on the training set.

Disclaimer: in general it won't be necessarily the case as PCA operates only on the predictors without "knowledge" of target variable.

![](images/desbois_pca_individuals_pc1_pc2.png){.r-stretch}

As suggested by Desbois, extract the first principal component (some help [here](https://stats.stackexchange.com/questions/460787/pcr-after-pca-with-mixed-data-how-to-extract-export-the-pcs-as-new-variables-i)), and use it as a Scoring function.

Today it will allow us to use a first Scoring function without introducing the Logistic Regression that you have not yet studied in class.

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
# https://stats.stackexchange.com/questions/460787/pcr-after-pca-with-mixed-data-how-to-extract-export-the-pcs-as-new-variables-i
# https://stats.stackexchange.com/questions/494866/how-to-explain-the-numerical-discrepancy-between-factominerpca-and-the-svd
# https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca

# Extracting first five Principal components from FactomineR object 
# Using FactomineR 'notation' (U,sv,V)
# (X nxp matrix of Desbois observations (only r1...r37), SVD decomposition X = U %*% diag(vs) %*% t(V), 
# where U: unitary matrix of left-singular vectors, vs: singular values, V: unitary matrix of left-singular vectors)

# SVD object from FactomineR res.pca$svd

# Principal components X %*% V = U %*% diag(vs) %*% t(V) %*% V = U %*% diag(vs) 
principal_components_1 <- res.pca$svd$U %*% diag(res.pca$svd$vs[1:5])

# Can also be directly extracted from:
principal_components_2 <- res.pca$ind$coord

# Or using PC = X %*% V (where X has been centered and scaled
# base R 'scale' uses a different scaling factor than FactomineR hence sqrt(n / n-1) correction)
X_for_PCA <- don_desbois %>% select(r1:r37) %>% as.matrix()
principal_components_3 <- sqrt(nrow(X_for_PCA) / (nrow(X_for_PCA) - 1)) * scale(X_for_PCA) %*% res.pca$svd$V
```

Below the score for each observation is the x-axis or first principal component:

```{r}
naive_score <-  bind_cols(don_desbois, as_tibble(principal_components_2))
ggplot(naive_score) +
    geom_point(aes(x = Dim.1, y = Dim.2, col = Y)) +
    scale_colour_manual(values = c("dodgerblue", "orange"))
  
```

Looking at this plot Desbois concludes that a simple classifier can be devised using the first PCA coordinate as a classifier (setting a threshold at $PC_1 > 0.02$):

![](images/desbois_pca_rule.png){.r-stretch}

Going further than Desbois and for illustrative purposes only, use $PC_1$ as a very naive Scoring function.

Plot below the ROC curve as defined in the Scoring part of the presentation (you can use package `ROCR`, but a simple for-loop for different cutoff values $s$ will do the job):

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
library("ROCR")    

# naive score
pred <- prediction(naive_score$Dim.1, naive_score$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange", 
     print.cutoffs.at = c(-2.5,-1,0.00,1,5), 
     cutoff.label.function = function(x) {paste0("              s = ",round(x,2))})
abline(0, 1) #add a 45 degree line
```

We follow closely the case study, but we will see that ROC curves are usually evaluated on a hold-out data set.

Then compare with a Scoring function obtained with Linear Discriminant Analysis (LDA). For a reminder on LDA see for example @hastie2009 [chap. 4.3 LDA, p. 103-111].

You can use Desbois variables selected in the article by a stepwise procedure (using Wilk's lambda[^1]). Selected variables are:

[^1]: We won’t describe the procedure here. A SPSS script is given in the article. It is not straightforward to reproduce it with R. I tried scripting manually the procedure and it works but is not very interesting.

![](images/desbois_lda_stepwise.png){.r-stretch}

So basically you can fit LDA with `r2+r3+r7+r14+r17+r18+r21+r32+r36` as predictors (I use `R` formula notation to ease your copy-paste).

Fit LDA (`MASS::lda`) with model specification of your choice. Then display model coefficients:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
lda_desbois <- MASS::lda(Y~r2+r3+r7+r14+r17+r18+r21+r32+r36, data=don_desbois)
round(lda_desbois$scaling,3)
```

Now compare the LDA Scoring function with the naive Score build with first component of PCA using a ROC curve (on training set):

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
# for ROCR
labels_desbois <- don_desbois$Y

# naive score
predictions_naive_score <- naive_score$Dim.1

naive_score_group <- naive_score %>%
  group_by(Y) %>%
  summarize(mean_score = mean(Dim.1), n = n())

cutoff_naive <- mean(naive_score_group$mean_score)

pred <- prediction(predictions_naive_score, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkolivegreen", 
     print.cutoffs.at = c(cutoff_naive), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
abline(0, 1) #add a 45 degree line

# lda
predictions_lda <- predict(lda_desbois)$x

lda_score_group <- bind_cols(don_desbois, predictions_lda) %>%
  group_by(Y) %>%
  summarize(mean_score = mean(LD1), n = n())

cutoff_lda <- mean(lda_score_group$mean_score)

pred <- prediction(predictions_lda, labels_desbois)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "plum4",
     print.cutoffs.at = c(cutoff_lda), 
     cutoff.label.function = function(x) {paste0("                s = ",round(x,2))})
legend(0.6,0.6,
       c('naive - PCA', 'LDA'),
       col=c("darkolivegreen", "plum4"),lwd=3)
```

As a very soft and intuitive introduction to logistic regression. First discretize a ratio $r$ of your choice.

-   Compute proportion of defaults among each class. For example using $0.01$ steps for ratio $r17$:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r, warning = FALSE, message = FALSE}
#| code-fold: true
class_width <- 0.01
don_desbois_binned <- don_desbois %>%
    mutate(r17_bins = cut(r17, breaks = seq(0, 0.2, class_width),
                              right = FALSE, dig.lab = 4, include.lowest = TRUE),
           min = floor(r17 / class_width) * class_width,
           max = if_else(r17 == 0 , 1, 
                         # customers with 0$ balance should belong to [0, width) class
                         # or be excluded
                         ceiling(r17 / class_width))  * class_width,
           max = if_else(min==max, (ceiling(r17 / class_width) + 1)  * class_width, max)) %>% 
    group_by(r17_bins, min, max, Y) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = Y, values_from = n) %>%
    replace_na(list(`0` = 0, `1` = 0)) %>% 
    mutate(`Mean(Y)` = round(`1` / (`1` + `0`), 4))
cat(simplermarkdown::md_table(don_desbois_binned %>% select(r17_class=r17_bins, `0`, `1`,`Mean(Y)` )))
```

-   Then using this table, plot an empirical conditional distribution of default given $r$ classes:

```{r}
#| echo: true
#| code-fold: show
#| warning: false

########## YOUR CODE HERE #####################
```

```{r}
#| code-fold: true
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```

```{r}
#| code-fold: true
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25)) +
 scale_y_continuous(breaks = c(0, 1))
```

This roughly corresponds to the intuitive introduction to the logistic regression model given in @hosmer2013 using a Coronary Heart Disease (CHD) event as $Y$ and AGE as $X$:

![](images/hosmer_lemeshow_chd3.png)

![](images/hosmer_lemeshow_chd4.png)
